---
permalink: concepts/rdma-concept.html
sidebar: sidebar
keywords: Remote Direct Memory Access, NFS over RDMA, RoCE, intercluster
summary: "NFS over RDMA and RDMA cluster interconnect leverage RDMA to improve performance in high-bandwidth workloads."
---
= RDMA overview
:icons: font
:imagesdir: ../media/

[.lead]
ONTAP's Remote Direct Memory Access (RDMA) offerings support latency-sensitive and high-bandwidth workloads. RDMA allows data to be copied directly between storage system memory and host system memory, circumventing CPU interruptions and overhead. 

== NFS over RDMA

Beginning with ONTAP 9.10.1, you can configure link:../nfs-rdma/index.html[NFS over RDMA] to enable the use of NVIDIA GPUDirect Storage for GPU-accelerated workloads on hosts with supported NVIDIA GPUs.

== RDMA cluster interconnect

RDMA cluster interconnect reduces latency, decreases failover times, and accelerates communication between nodes in a cluster.

Beginning with ONTAP 9.10.1, cluster interconnect RDMA is supported for certain hardware systems when used with X1151A cluster NICs. Beginning with ONTAP 9.13.1, X91153A NICs also support cluster interconnect RDMA. Consult the table to learn what systems are supported in different ONTAP releases. 

[options="header"]
|===
 | Systems | Supported ONTAP versions
a| 
* A400 
* ASA A400 
| ONTAP 9.10.1 and later 
a|
* AFF A900 
* ASA A900 
* FAS9500 
| ONTAP 9.13.1 and later 
|===

Given the appropriate storage system set up, no additional configuration is needed to use RDMA interconnect. 

// 2024 apr 11, GITHUB issue 1321 reverting changes in 878 for e-series link and redundancy
// 2024 feb 01, ONTAPDOC-1337
// 18 oct 2023, ontapdoc-1138
// 3 August 2023, ontap-issues-878