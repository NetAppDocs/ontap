---
permalink: revert/task_verify_health.html
sidebar: sidebar
keywords: ontap, revert, reverting, reversion, downgrade, downgrading, verifying, verify, cluster, health
summary: 'After you revert an ONTAP cluster you should verify that the nodes are healthy and eligible to participate in the cluster, and that the cluster is in quorum.'
---

= Verify cluster and storage health after an ONTAP revert 
:icons: font
:imagesdir: ../media/

[.lead]
After you revert an ONTAP cluster, you should verify that the nodes are healthy and eligible to participate in the cluster, and that the cluster is in quorum. You should also verify the status of your disks, aggregates, and volumes.


== Verify cluster health

.Steps

. Verify that the nodes in the cluster are online and are eligible to participate in the cluster: 
+
[source,cli]
----
cluster show
----
+
In this example, the cluster is healthy and all nodes are eligible to participate in the cluster.
+
----
cluster1::> cluster show
Node                  Health  Eligibility
--------------------- ------- ------------
node0                 true    true
node1                 true    true
----
+
If any node is unhealthy or ineligible, check EMS logs for errors and take corrective action.

. Set the privilege level to advanced: 
+
[source,cli]
----
set -privilege advanced
----
+
Enter `y` to continue.

. Verify the configuration details for each RDB process.
** The relational database epoch and database epochs should match for each node.
** The per-ring quorum master should be the same for all nodes.
+
Note that each ring might have a different quorum master.
+
[cols=2*,options="header"]
|===
| To display this RDB process...| Enter this command...
a|
Management application
a|
[source,cli]
----
cluster ring show -unitname mgmt
----

a|
Volume location database
a|
[source,cli]
----
cluster ring show -unitname vldb
----

a|
Virtual-Interface manager
a|
[source,cli]
----
cluster ring show -unitname vifmgr
----

a|
SAN management daemon
a|
[source,cli]
----
cluster ring show -unitname bcomd
----
|===
+
This example shows the volume location database process:
+
----
cluster1::*> cluster ring show -unitname vldb
Node      UnitName Epoch    DB Epoch DB Trnxs Master    Online
--------- -------- -------- -------- -------- --------- ---------
node0     vldb     154      154      14847    node0     master
node1     vldb     154      154      14847    node0     secondary
node2     vldb     154      154      14847    node0     secondary
node3     vldb     154      154      14847    node0     secondary
4 entries were displayed.
----

. Return to the admin privilege level: 
+
[source,cli]
----
set -privilege admin
----

. If you are operating in a SAN environment, verify that each node is in a SAN quorum: 
+
[source,cli]
----
event log show  -severity informational -message-name scsiblade.*
----
+
The most recent scsiblade event message for each node should indicate that the scsi-blade is in quorum.
+
----
cluster1::*> event log show  -severity informational -message-name scsiblade.*
Time             Node       Severity       Event
---------------  ---------- -------------- ---------------------------
MM/DD/YYYY TIME  node0      INFORMATIONAL  scsiblade.in.quorum: The scsi-blade ...
MM/DD/YYYY TIME  node1      INFORMATIONAL  scsiblade.in.quorum: The scsi-blade ...
----

.Related information

link:../system-admin/index.html[System administration]

== Verify storage health

After you revert or downgrade a cluster, you should verify the status of your disks, aggregates, and volumes.

.Steps

. Verify disk status:
+
[cols=2*,options="header"]
|===
| To check for...| Do this...
a|
Broken disks
a|

.. Display any broken disks: 
+
[source,cli]
----
storage disk show -state broken
----

.. Remove or replace any broken disks.

a|
Disks undergoing maintenance or reconstruction
a|

.. Display any disks in maintenance, pending, or reconstructing states: 
+
[source,cli]
----
storage disk show -state maintenance\|pending\|reconstructing
----

.. Wait for the maintenance or reconstruction operation to finish before proceeding.
|===

. Verify that all aggregates are online by displaying the state of physical and logical storage, including storage aggregates: 
+
[source,cli]
----
storage aggregate show -state !online
----
+
This command displays the aggregates that are _not_ online. All aggregates must be online before and after performing a major upgrade or reversion.
+
----
cluster1::> storage aggregate show -state !online
There are no entries matching your query.
----

. Verify that all volumes are online by displaying any volumes that are _not_ online: 
+
[source,cli]
----
volume show -state !online
----
+
All volumes must be online before and after performing a major upgrade or reversion.
+
----
cluster1::> volume show -state !online
There are no entries matching your query.
----

. Verify that there are no inconsistent volumes: 
+
[source,cli]
----
volume show -is-inconsistent true
----
+
See the Knowledge Base article link:https://kb.netapp.com/Advice_and_Troubleshooting/Data_Storage_Software/ONTAP_OS/Volume_Showing_WAFL_Inconsistent[Volume Showing WAFL Inconsistent] on how to address the inconsistent volumes.

.Related information

link:../disks-aggregates/index.html[Disk and aggregate management]

// 2022-04-25, BURT 1454366

== Verify client access (SMB and NFS)

For the configured protocols, test access from SMB and NFS clients to verify that the cluster is accessible.

// 2024 Dec 05, Jira 2563
// 4 Feb 2022, BURT 1451789 